{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bcf37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[인공지능] 키워드로 뉴스 기사를 검색합니다...\n",
      "Checking keywords... '인공지능'\n",
      "https://search.naver.com/search.naver?where=news&query=인공지능&start=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "더 이상 새로운 관련 기사를 찾을 수 없습니다.\n",
      "조건에 맞는 네이버 뉴스 기사를 찾지 못했습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 설정 구간\n",
    "# ==========================================\n",
    "SEARCH_KEYWORD = \"인공지능\"  # 검색할 키워드 입력\n",
    "MAX_ARTICLES = 5          # 수집할 기사 최대 개수 (네이버 뉴스 호스팅 링크 기준)\n",
    "# ==========================================\n",
    "\n",
    "def get_news_urls(keyword, limit=5):\n",
    "    \"\"\"\n",
    "    네이버 검색 결과에서 '네이버 뉴스' 플랫폼에 호스팅된 기사 URL만 추출합니다.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    page = 1  # start 파라미터 (1, 11, 21...)\n",
    "    \n",
    "    print(f\"Checking keywords... '{keyword}'\")\n",
    "    \n",
    "    while len(urls) < limit:\n",
    "        # 네이버 뉴스 검색 URL\n",
    "        search_url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&start={page}\"\n",
    "        print(search_url)\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 검색 결과 중 '네이버뉴스' 아이콘이 있는 링크 찾기\n",
    "            # div.info_group > a.info 클래스를 가진 태그가 네이버 뉴스 링크\n",
    "            links = soup.select('div.info_group > a.info')\n",
    "            \n",
    "            found_current_page = False\n",
    "            for link in links:\n",
    "                if '네이버뉴스' in link.text and 'news.naver.com' in link['href']:\n",
    "                    url = link['href']\n",
    "                    if url not in urls:\n",
    "                        urls.append(url)\n",
    "                        print(f\"기사 발견: {url}\")\n",
    "                        found_current_page = True\n",
    "                    \n",
    "                    if len(urls) >= limit:\n",
    "                        break\n",
    "            \n",
    "            if not found_current_page:\n",
    "                print(\"더 이상 새로운 관련 기사를 찾을 수 없습니다.\")\n",
    "                break\n",
    "                \n",
    "            page += 10 # 다음 페이지로 (네이버 검색은 10 단위)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"검색 중 에러 발생: {e}\")\n",
    "            break\n",
    "            \n",
    "    return urls[:limit]\n",
    "\n",
    "def get_news_ids(url):\n",
    "    \"\"\"\n",
    "    URL에서 언론사 ID(oid)와 기사 ID(aid)를 추출합니다.\n",
    "    \"\"\"\n",
    "    # URL 패턴 매칭 (PC 및 모바일 URL 모두 대응)\n",
    "    match = re.search(r'(?:oid=|article\\/)(\\d+)(?:&aid=|\\/)(\\d+)', url)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return None, None\n",
    "\n",
    "def get_naver_comments(url):\n",
    "    \"\"\"\n",
    "    특정 뉴스 기사의 댓글을 수집합니다.\n",
    "    \"\"\"\n",
    "    oid, aid = get_news_ids(url)\n",
    "    if not oid or not aid:\n",
    "        print(f\"건너뜀: 올바른 네이버 뉴스 URL이 아닙니다. ({url})\")\n",
    "        return []\n",
    "\n",
    "    # 네이버 댓글 API 엔드포인트\n",
    "    api_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "        'Referer': url\n",
    "    }\n",
    "\n",
    "    comments = []\n",
    "    page = 1\n",
    "    \n",
    "    print(f\"댓글 수집 시작: {url}\")\n",
    "\n",
    "    while True:\n",
    "        # API 파라미터 설정\n",
    "        params = {\n",
    "            'ticket': 'news',          # 일반 뉴스: news\n",
    "            'templateId': 'default_society',\n",
    "            'pool': 'cbox5',\n",
    "            'lang': 'ko',\n",
    "            'country': 'KR',\n",
    "            'objectId': f'news{oid},{aid}',\n",
    "            'pageSize': 100,\n",
    "            'page': page,\n",
    "            'sort': 'favorite',        # favorite(순공감순), new(최신순)\n",
    "            'initialize': 'true' if page == 1 else 'false',\n",
    "            'useAltSort': 'true',\n",
    "            'replyPageSize': 20\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=headers, params=params)\n",
    "            content = response.text\n",
    "            \n",
    "            # JSONP or JSON parsing\n",
    "            if '(' in content and ')' in content:\n",
    "                start = content.find('(') + 1\n",
    "                end = content.rfind(')')\n",
    "                json_data = json.loads(content[start:end])\n",
    "            else:\n",
    "                json_data = json.loads(content)\n",
    "\n",
    "            if not json_data.get('success'):\n",
    "                # 댓글 기능이 없는 기사이거나 오류 발생 시\n",
    "                break\n",
    "                \n",
    "            result = json_data.get('result', {})\n",
    "            comment_list = result.get('commentList', [])\n",
    "            \n",
    "            if not comment_list:\n",
    "                break\n",
    "\n",
    "            for item in comment_list:\n",
    "                comments.append({\n",
    "                    '기사URL': url,\n",
    "                    '작성자': item['userName'],\n",
    "                    '내용': item['contents'],\n",
    "                    '날짜': item['regTime'],\n",
    "                    '공감수': item['sympathyCount'],\n",
    "                    '비공감수': item['antipathyCount']\n",
    "                })\n",
    "            \n",
    "            # 페이징 종료 조건\n",
    "            total_count = result.get('count', {}).get('comment', 0)\n",
    "            if len(comments) >= total_count:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(0.3) # 딜레이\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "            break\n",
    "            \n",
    "    print(f\" -> {len(comments)}개 수집 완료\")\n",
    "    return comments\n",
    "\n",
    "\n",
    "print(f\"[{SEARCH_KEYWORD}] 키워드로 뉴스 기사를 검색합니다...\")\n",
    "news_urls = get_news_urls(SEARCH_KEYWORD, MAX_ARTICLES)\n",
    "\n",
    "if news_urls:\n",
    "    print(f\"\\n총 {len(news_urls)}개의 기사에서 댓글 수집을 진행합니다.\\n\" + \"=\"*50)\n",
    "\n",
    "    all_comments = []\n",
    "\n",
    "    # 2. 각 URL별 댓글 수집\n",
    "    for url in news_urls:\n",
    "        article_comments = get_naver_comments(url)\n",
    "        if article_comments:\n",
    "            all_comments.extend(article_comments)\n",
    "        time.sleep(1) # 기사 간 차단 방지 딜레이\n",
    "\n",
    "    # 3. 결과 저장\n",
    "    if all_comments:\n",
    "        df = pd.DataFrame(all_comments)\n",
    "        \n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_name = f\"naver_search_{SEARCH_KEYWORD}_{timestamp}.csv\"\n",
    "        \n",
    "        # 컬럼 순서 지정\n",
    "        cols = ['기사URL', '작성자', '내용', '날짜', '공감수', '비공감수']\n",
    "        df = df[cols]\n",
    "        \n",
    "        df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n[완료] 총 {len(df)}개의 댓글이 '{file_name}' 파일로 저장되었습니다.\")\n",
    "    else:\n",
    "        print(\"\\n수집된 댓글이 없습니다.\")\n",
    "else:\n",
    "    print(\"조건에 맞는 네이버 뉴스 기사를 찾지 못했습니다.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
