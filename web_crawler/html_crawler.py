import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse
import re

def download_pdfs_from_url(base_url, download_dir='downloaded_pdfs'):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ëª¨ë“  PDF íŒŒì¼ì„ ì°¾ì•„ ì§€ì •ëœ ë””ë ‰í„°ë¦¬ì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”— ì›¹í˜ì´ì§€ ì ‘ì† ì‹œë„: {base_url}")
    
    # 1. HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
    try:
        response = requests.get(base_url, timeout=10)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì›¹í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
        return

    # 2. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ ìƒì„±
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)
        print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ë””ë ‰í„°ë¦¬ ìƒì„±: '{download_dir}'")

    pdf_count = 0
    
    # 3. ëª¨ë“  <a> íƒœê·¸(ë§í¬) ì°¾ê¸°
    for link in soup.find_all('a', href=True):
        href = link.get('href')
        
        # PDF íŒŒì¼ ë§í¬ í•„í„°ë§ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ .pdfë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸)
        if href.lower().endswith('.pdf'):
            # 4. ì ˆëŒ€ URLë¡œ ë³€í™˜
            pdf_url = urljoin(base_url, href)
            
            # íŒŒì¼ ì´ë¦„ ê²°ì •
            # URLì˜ ë§ˆì§€ë§‰ ê²½ë¡œë¥¼ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
            path = urlparse(pdf_url).path
            filename = os.path.basename(path).split('?')[0] # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
            
            # íŒŒì¼ ì´ë¦„ ì •ë¦¬ (ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ì ì œê±°)
            filename = re.sub(r'[^\w\-_\. ]', '_', filename)
            
            if not filename:
                filename = f"unnamed_pdf_{pdf_count}.pdf" # íŒŒì¼ ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„

            save_path = os.path.join(download_dir, filename)

            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì¸ì§€ í™•ì¸ (ì„ íƒ ì‚¬í•­)
            if os.path.exists(save_path):
                print(f"  ğŸ‘‰ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {filename}")
                continue

            print(f"â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ ì‹œë„: {filename} from {pdf_url}")
            
            # 5. PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            try:
                file_response = requests.get(pdf_url, stream=True, timeout=20)
                file_response.raise_for_status()

                # 6. íŒŒì¼ ì €ì¥
                with open(save_path, 'wb') as f:
                    # í° íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì“°ê¸°
                    for chunk in file_response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
                pdf_count += 1
            
            except requests.exceptions.RequestException as e:
                print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
                
    if pdf_count > 0:
        print(f"\nğŸ‰ ì´ {pdf_count}ê°œì˜ PDF íŒŒì¼ì„ '{download_dir}' ë””ë ‰í„°ë¦¬ì— ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì›¹í˜ì´ì§€ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    TARGET_URL = 'https://sites.google.com/cs.washington.edu/xai/schedule-reading'
    DOWNLOAD_FOLDER = 'CSE599_UW_ExplainableAI'
    
    download_pdfs_from_url(TARGET_URL, DOWNLOAD_FOLDER)
